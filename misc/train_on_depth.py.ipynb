{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train on Depth\n",
    "reference from \n",
    "\n",
    "https://github.com/asap-report/carla/blob/racetrack/PythonClient/racetrack/train_on_depth.py\n",
    "https://github.com/ManajitPal/DeepLearningForSelfDrivingCars/blob/master/self_driving_car.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if data exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 981 data points\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"/home/michael/Desktop/projects/ROAR/data/custom\")\n",
    "left_depth_dir = data_dir / \"left\"\n",
    "center_depth_dir = data_dir / \"center\"\n",
    "right_depth_dir = data_dir / \"right\"\n",
    "veh_state_dir = data_dir / \"state\"\n",
    "\n",
    "assert data_dir.exists(), \"data dir does not exist\"\n",
    "assert left_depth_dir.exists(), \"left_depth_dir does not exist\"\n",
    "assert center_depth_dir.exists(), \"left_depth_dir does not exist\"\n",
    "assert right_depth_dir.exists(), \"left_depth_dir does not exist\"\n",
    "assert veh_state_dir.exists(), \"veh_state_dir does not exist\"\n",
    "\n",
    "left_depth_paths = [p for p in sorted(left_depth_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "center_depth_paths = [p for p in sorted(center_depth_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "right_depth_paths = [p for p in sorted(right_depth_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "veh_state_paths = [p for p in sorted(veh_state_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "\n",
    "assert len(left_depth_paths) == len(center_depth_paths)== len(right_depth_paths)==len(veh_state_paths), \"not the same size of X and y\"\n",
    "print(f\"Found { len(left_depth_paths)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael/Desktop/projects/ROAR/data/custom/left/frame_09_19_2021_22_12_07_265034.npy\n",
      "/home/michael/Desktop/projects/ROAR/data/custom/center/frame_09_19_2021_22_12_07_265034.npy\n",
      "/home/michael/Desktop/projects/ROAR/data/custom/right/frame_09_19_2021_22_12_07_265034.npy\n",
      "/home/michael/Desktop/projects/ROAR/data/custom/state/frame_09_19_2021_22_12_07_265034.npy\n"
     ]
    }
   ],
   "source": [
    "print(left_depth_paths[0])\n",
    "print(center_depth_paths[0])\n",
    "print(right_depth_paths[0])\n",
    "print(veh_state_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/envs/ROAR/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py:331: MatplotlibDeprecationWarning: \n",
      "The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead.\n",
      "  if ax.is_first_col():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'steering'}>,\n",
       "        <AxesSubplot:title={'center':'throttle'}>]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "throttles, steerings = [], []\n",
    "for st_path in veh_state_paths:\n",
    "    array = np.load(st_path)\n",
    "    throttles.append(array[-2])\n",
    "    steerings.append(array[-1])\n",
    "df = pd.DataFrame(\n",
    "    data={\"throttle\":throttles, \"steering\":steerings}\n",
    ")\n",
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Do data augmentation\n",
    "\n",
    "Let's only train on steering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, samples, left_depth_dir, center_depth_dir, right_depth_dir, state_dir, transform=None):\n",
    "\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "        self.left_depth_dir: Path = left_depth_dir\n",
    "        self.right_depth_dir: Path = right_depth_dir\n",
    "        self.center_depth_dir: Path = center_depth_dir\n",
    "        self.state_dir: Path = state_dir\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      \n",
    "        sample_path = self.samples[index]\n",
    "        left_depth_path = self.left_depth_dir / sample_path\n",
    "        center_depth_path = self.center_depth_dir / sample_path\n",
    "        right_depth_path = self.right_depth_dir / sample_path\n",
    "        veh_state_path = self.state_dir / sample_path\n",
    "        \n",
    "        left_depth = np.load(left_depth_path)\n",
    "        right_depth = np.load(left_depth_path)\n",
    "        center_depth = np.load(right_depth_path)\n",
    "        veh_state = np.load(veh_state_path)\n",
    "        \n",
    "        steering_angle_center = veh_state[-1]\n",
    "        steering_angle_left = steering_angle_center + 0.2\n",
    "        steering_angle_right = steering_angle_center - 0.2\n",
    "        return (center_depth, steering_angle_center), \\\n",
    "               (left_depth, steering_angle_left), \\\n",
    "               (right_depth, steering_angle_right)\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 1,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 4}\n",
    "samples = [Path(p).name for p in veh_state_paths]\n",
    "samples = samples[:len(samples) - len(samples)%params['batch_size'] ]\n",
    "dataset = Dataset(samples=samples, \n",
    "                  left_depth_dir = left_depth_dir, \n",
    "                  center_depth_dir=center_depth_dir, \n",
    "                  right_depth_dir=right_depth_dir,\n",
    "                  state_dir = veh_state_dir)\n",
    "data_generator = data.DataLoader(dataset, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Step6: Check the device and define function to move tensors to that device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print('device is: ', device)\n",
    "\n",
    "def toDevice(datas, device):\n",
    "    imgs, angles = datas\n",
    "    return imgs.float().to(device), angles.float().to(device)\n",
    "def to_tensor(datas, device):\n",
    "    imgs, angles = datas\n",
    "    return imgs.float().to(device), angles.float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NvidiaModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NvidiaModel, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 24, 5, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(24, 36, 5, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(36, 48, 5, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(48, 64, 3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=404736, out_features=50),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=50, out_features=10),\n",
    "            nn.Linear(in_features=10, out_features=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):  \n",
    "        input = input.view(input.size(0), 1, 600, 800)\n",
    "        output = self.conv_layers(input)\n",
    "        output = torch.flatten(output)\n",
    "        output = self.linear_layers(output)\n",
    "        return output\n",
    "model = NvidiaModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring Epoch 0\n",
      "    At iteration 0 -> loss: 2.1784337796270847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/envs/ROAR/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 10 -> loss: 6.213915099186124\n",
      "    At iteration 20 -> loss: 7.575714926206274\n",
      "    At iteration 30 -> loss: 8.471685166059615\n",
      "    At iteration 40 -> loss: 9.481662773174321\n",
      "    At iteration 50 -> loss: 10.3687939569445\n",
      "    At iteration 60 -> loss: 11.244889974019316\n",
      "    At iteration 70 -> loss: 12.360507460776716\n",
      "    At iteration 80 -> loss: 13.545927020342788\n",
      "    At iteration 90 -> loss: 14.35728020343413\n",
      "    At iteration 100 -> loss: 15.285666734214828\n",
      "    At iteration 110 -> loss: 16.2659329703107\n",
      "    At iteration 120 -> loss: 17.19391660254007\n",
      "    At iteration 130 -> loss: 18.031216289120152\n",
      "    At iteration 140 -> loss: 18.968936674743986\n",
      "    At iteration 150 -> loss: 20.02398940302794\n",
      "    At iteration 160 -> loss: 20.95330499169779\n",
      "    At iteration 170 -> loss: 22.15693573006837\n",
      "    At iteration 180 -> loss: 22.986620772059666\n",
      "    At iteration 190 -> loss: 24.20127797743004\n",
      "    At iteration 200 -> loss: 25.041450888359634\n",
      "    At iteration 210 -> loss: 25.885540536166502\n",
      "    At iteration 220 -> loss: 26.815696277915833\n",
      "    At iteration 230 -> loss: 27.80051528681912\n",
      "    At iteration 240 -> loss: 28.69894390144151\n",
      "    At iteration 250 -> loss: 30.08301037890311\n",
      "    At iteration 260 -> loss: 30.94115055607159\n",
      "    At iteration 270 -> loss: 31.814580219034326\n",
      "    At iteration 280 -> loss: 32.748347735636855\n",
      "    At iteration 290 -> loss: 33.71689967799733\n",
      "    At iteration 300 -> loss: 34.54416762839077\n",
      "    At iteration 310 -> loss: 35.43665962107423\n",
      "    At iteration 320 -> loss: 36.37075000706369\n",
      "    At iteration 330 -> loss: 37.24860730340043\n",
      "    At iteration 340 -> loss: 38.099864399044975\n",
      "    At iteration 350 -> loss: 38.952755871127295\n",
      "    At iteration 360 -> loss: 39.798610583486436\n",
      "    At iteration 370 -> loss: 40.675366176568616\n",
      "    At iteration 380 -> loss: 41.56588235667715\n",
      "    At iteration 390 -> loss: 42.43227642423955\n",
      "    At iteration 400 -> loss: 43.2764588309272\n",
      "    At iteration 410 -> loss: 44.69513931172497\n",
      "    At iteration 420 -> loss: 45.53941225916361\n",
      "    At iteration 430 -> loss: 46.38502067395467\n",
      "    At iteration 440 -> loss: 47.24839397560555\n",
      "    At iteration 450 -> loss: 48.090862492423696\n",
      "    At iteration 460 -> loss: 48.95716431103352\n",
      "    At iteration 470 -> loss: 49.795161508759065\n",
      "    At iteration 480 -> loss: 50.65517268284168\n",
      "    At iteration 490 -> loss: 51.71747714340165\n",
      "    At iteration 500 -> loss: 52.6401249580116\n",
      "    At iteration 510 -> loss: 53.472099958476406\n",
      "    At iteration 520 -> loss: 54.34627002779723\n",
      "    At iteration 530 -> loss: 55.22103658917274\n",
      "    At iteration 540 -> loss: 56.11601191157331\n",
      "    At iteration 550 -> loss: 56.95449055325605\n",
      "    At iteration 560 -> loss: 57.904158954162426\n",
      "    At iteration 570 -> loss: 58.75319359437482\n",
      "    At iteration 580 -> loss: 59.61005983830684\n",
      "    At iteration 590 -> loss: 60.49802368146722\n",
      "    At iteration 600 -> loss: 61.38844349418743\n",
      "    At iteration 610 -> loss: 62.29402557606319\n",
      "    At iteration 620 -> loss: 63.1671088387093\n",
      "    At iteration 630 -> loss: 64.0483867703306\n",
      "    At iteration 640 -> loss: 64.9099015058536\n",
      "    At iteration 650 -> loss: 65.78734443913793\n",
      "    At iteration 660 -> loss: 66.64652534753888\n",
      "    At iteration 670 -> loss: 67.51226174028754\n",
      "    At iteration 680 -> loss: 68.39444282754896\n",
      "    At iteration 690 -> loss: 69.24617977797872\n",
      "    At iteration 700 -> loss: 70.16083853723369\n",
      "    At iteration 710 -> loss: 71.03377454580861\n",
      "    At iteration 720 -> loss: 71.90326745055663\n",
      "    At iteration 730 -> loss: 72.82448642748992\n",
      "    At iteration 740 -> loss: 73.70709621474148\n",
      "    At iteration 750 -> loss: 74.62022582050182\n",
      "    At iteration 760 -> loss: 75.49874980724476\n",
      "    At iteration 770 -> loss: 76.41300047044584\n",
      "    At iteration 780 -> loss: 77.31831541102306\n",
      "    At iteration 790 -> loss: 78.15376137686431\n",
      "    At iteration 800 -> loss: 79.02757466714425\n",
      "    At iteration 810 -> loss: 79.86428469351918\n",
      "    At iteration 820 -> loss: 81.07077678331612\n",
      "    At iteration 830 -> loss: 82.05466460366065\n",
      "    At iteration 840 -> loss: 83.12406264852021\n",
      "    At iteration 850 -> loss: 84.05348821316207\n",
      "    At iteration 860 -> loss: 84.90742144267611\n",
      "    At iteration 870 -> loss: 85.78279788789919\n",
      "    At iteration 880 -> loss: 86.67286301224036\n",
      "    At iteration 890 -> loss: 87.5866974373411\n",
      "    At iteration 900 -> loss: 88.42790117277029\n",
      "    At iteration 910 -> loss: 89.2959686002079\n",
      "    At iteration 920 -> loss: 90.17659918292809\n",
      "    At iteration 930 -> loss: 91.06300594398014\n",
      "    At iteration 940 -> loss: 91.89464128318393\n",
      "    At iteration 950 -> loss: 92.77687455115408\n",
      "    At iteration 960 -> loss: 93.80220015116394\n",
      "    At iteration 970 -> loss: 94.73832980889021\n",
      "    At iteration 980 -> loss: 95.58794335628265\n",
      "Staring Epoch 1\n",
      "    At iteration 0 -> loss: 0.16140418010763824\n",
      "    At iteration 10 -> loss: 1.044454891223495\n",
      "    At iteration 20 -> loss: 1.9074222995364494\n",
      "    At iteration 30 -> loss: 2.7806577245515314\n",
      "    At iteration 40 -> loss: 3.657079042823682\n",
      "    At iteration 50 -> loss: 4.511104225183374\n",
      "    At iteration 60 -> loss: 5.421721770641454\n",
      "    At iteration 70 -> loss: 6.284858052469701\n",
      "    At iteration 80 -> loss: 7.149461445365944\n",
      "    At iteration 90 -> loss: 8.0776433165193\n",
      "    At iteration 100 -> loss: 8.960066198414154\n",
      "    At iteration 110 -> loss: 9.872347682779093\n",
      "    At iteration 120 -> loss: 10.739441550168578\n",
      "    At iteration 130 -> loss: 11.603080789772378\n",
      "    At iteration 140 -> loss: 12.457220269359652\n",
      "    At iteration 150 -> loss: 13.318125283425445\n",
      "    At iteration 160 -> loss: 14.284700542353505\n",
      "    At iteration 170 -> loss: 15.170204678364655\n",
      "    At iteration 180 -> loss: 16.134098245860343\n",
      "    At iteration 190 -> loss: 17.003554401144186\n",
      "    At iteration 200 -> loss: 17.88939737288075\n",
      "    At iteration 210 -> loss: 18.773480422510886\n",
      "    At iteration 220 -> loss: 19.663106113984952\n",
      "    At iteration 230 -> loss: 20.652773653120505\n",
      "    At iteration 240 -> loss: 21.48261638762255\n",
      "    At iteration 250 -> loss: 22.343815293251502\n",
      "    At iteration 260 -> loss: 23.208983340127126\n",
      "    At iteration 270 -> loss: 24.13779917473844\n",
      "    At iteration 280 -> loss: 25.016598789380488\n",
      "    At iteration 290 -> loss: 26.078191400380867\n",
      "    At iteration 300 -> loss: 26.907376615398654\n",
      "    At iteration 310 -> loss: 27.778014916376243\n",
      "    At iteration 320 -> loss: 29.233572633939595\n",
      "    At iteration 330 -> loss: 30.12629361509139\n",
      "    At iteration 340 -> loss: 31.113847598588883\n",
      "    At iteration 350 -> loss: 32.260322208923505\n",
      "    At iteration 360 -> loss: 33.24364557908823\n",
      "    At iteration 370 -> loss: 34.12941876723173\n",
      "    At iteration 380 -> loss: 35.010018449924\n",
      "    At iteration 390 -> loss: 35.98760788987395\n",
      "    At iteration 400 -> loss: 36.84649590091815\n",
      "    At iteration 410 -> loss: 38.12516031216749\n",
      "    At iteration 420 -> loss: 38.97385337795661\n",
      "    At iteration 430 -> loss: 39.88805283173517\n",
      "    At iteration 440 -> loss: 40.75814176183051\n",
      "    At iteration 450 -> loss: 41.649989684726485\n",
      "    At iteration 460 -> loss: 42.53778994811294\n",
      "    At iteration 470 -> loss: 43.39361096831565\n",
      "    At iteration 480 -> loss: 44.25050260100691\n",
      "    At iteration 490 -> loss: 45.10504165989324\n",
      "    At iteration 500 -> loss: 45.99801925088605\n",
      "    At iteration 510 -> loss: 46.87154215110923\n",
      "    At iteration 520 -> loss: 48.09833400807975\n",
      "    At iteration 530 -> loss: 48.959507883788554\n",
      "    At iteration 540 -> loss: 49.82740634376592\n",
      "    At iteration 550 -> loss: 50.73306137283931\n",
      "    At iteration 560 -> loss: 51.60334176414677\n",
      "    At iteration 570 -> loss: 52.468748530635146\n",
      "    At iteration 580 -> loss: 53.3453572109977\n",
      "    At iteration 590 -> loss: 54.18841346702926\n",
      "    At iteration 600 -> loss: 55.125895966192054\n",
      "    At iteration 610 -> loss: 56.08967655613947\n",
      "    At iteration 620 -> loss: 56.93638846977934\n",
      "    At iteration 630 -> loss: 57.83440184984443\n",
      "    At iteration 640 -> loss: 58.73769190902909\n",
      "    At iteration 650 -> loss: 59.953238253552406\n",
      "    At iteration 660 -> loss: 60.839206307000275\n",
      "    At iteration 670 -> loss: 61.76842332503271\n",
      "    At iteration 680 -> loss: 62.631381551765465\n",
      "    At iteration 690 -> loss: 63.503704146832554\n",
      "    At iteration 700 -> loss: 64.38699312752703\n",
      "    At iteration 710 -> loss: 65.24207731522017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 720 -> loss: 66.08289101267326\n",
      "    At iteration 730 -> loss: 67.19314539369987\n",
      "    At iteration 740 -> loss: 68.07356183675827\n",
      "    At iteration 750 -> loss: 68.93350549194136\n",
      "    At iteration 760 -> loss: 69.81192427034446\n",
      "    At iteration 770 -> loss: 70.6823906960357\n",
      "    At iteration 780 -> loss: 71.54184498501346\n",
      "    At iteration 790 -> loss: 72.42136956904724\n",
      "    At iteration 800 -> loss: 73.33304774207309\n",
      "    At iteration 810 -> loss: 74.20983778239125\n",
      "    At iteration 820 -> loss: 75.07331753346799\n",
      "    At iteration 830 -> loss: 76.44243993307876\n",
      "    At iteration 840 -> loss: 77.36910515082806\n",
      "    At iteration 850 -> loss: 78.2107460913289\n",
      "    At iteration 860 -> loss: 79.08015812380697\n",
      "    At iteration 870 -> loss: 79.92616250716296\n",
      "    At iteration 880 -> loss: 80.824210193943\n",
      "    At iteration 890 -> loss: 81.72626379423659\n",
      "    At iteration 900 -> loss: 82.5961493437679\n",
      "    At iteration 910 -> loss: 83.50376756974569\n",
      "    At iteration 920 -> loss: 84.40377111104593\n",
      "    At iteration 930 -> loss: 85.28306543768761\n",
      "    At iteration 940 -> loss: 86.1879211389851\n",
      "    At iteration 950 -> loss: 87.05032546517307\n",
      "    At iteration 960 -> loss: 87.90933660678141\n",
      "    At iteration 970 -> loss: 88.8194305956263\n",
      "    At iteration 980 -> loss: 89.66808567881529\n",
      "Staring Epoch 2\n",
      "    At iteration 0 -> loss: 0.08784999023191631\n",
      "    At iteration 10 -> loss: 0.9945672685898899\n",
      "    At iteration 20 -> loss: 1.8546749711107697\n",
      "    At iteration 30 -> loss: 2.827376446171373\n",
      "    At iteration 40 -> loss: 3.787899494587153\n",
      "    At iteration 50 -> loss: 5.014600941241413\n",
      "    At iteration 60 -> loss: 5.854072505758639\n",
      "    At iteration 70 -> loss: 6.733203387880538\n",
      "    At iteration 80 -> loss: 7.636205077429084\n",
      "    At iteration 90 -> loss: 8.511884215902144\n",
      "    At iteration 100 -> loss: 9.5323499522614\n",
      "    At iteration 110 -> loss: 10.470665311607634\n",
      "    At iteration 120 -> loss: 11.412017827845746\n",
      "    At iteration 130 -> loss: 12.352838522499413\n",
      "    At iteration 140 -> loss: 13.217591500120223\n",
      "    At iteration 150 -> loss: 14.180898527650614\n",
      "    At iteration 160 -> loss: 15.091737174407998\n",
      "    At iteration 170 -> loss: 15.975100106764247\n",
      "    At iteration 180 -> loss: 16.868424840145877\n",
      "    At iteration 190 -> loss: 17.76570857937068\n",
      "    At iteration 200 -> loss: 19.208574275346024\n",
      "    At iteration 210 -> loss: 20.053942474805723\n",
      "    At iteration 220 -> loss: 20.939156200267377\n",
      "    At iteration 230 -> loss: 21.791660800394226\n",
      "    At iteration 240 -> loss: 22.66048944973582\n",
      "    At iteration 250 -> loss: 23.524106376622427\n",
      "    At iteration 260 -> loss: 24.414059756723788\n",
      "    At iteration 270 -> loss: 25.251644730424147\n",
      "    At iteration 280 -> loss: 26.105883557795693\n",
      "    At iteration 290 -> loss: 26.982184952134645\n",
      "    At iteration 300 -> loss: 28.44254134744793\n",
      "    At iteration 310 -> loss: 29.772430129302535\n",
      "    At iteration 320 -> loss: 30.60818165045086\n",
      "    At iteration 330 -> loss: 31.499433991439872\n",
      "    At iteration 340 -> loss: 32.39107996063714\n",
      "    At iteration 350 -> loss: 33.24582028521283\n",
      "    At iteration 360 -> loss: 34.07909477131645\n",
      "    At iteration 370 -> loss: 34.96253983830567\n",
      "    At iteration 380 -> loss: 35.84108294367499\n",
      "    At iteration 390 -> loss: 36.68779775216419\n",
      "    At iteration 400 -> loss: 37.55005856529053\n",
      "    At iteration 410 -> loss: 38.422046013197104\n",
      "    At iteration 420 -> loss: 39.331483765744196\n",
      "    At iteration 430 -> loss: 40.193823283707296\n",
      "    At iteration 440 -> loss: 41.13749615035331\n",
      "    At iteration 450 -> loss: 41.98879694926281\n",
      "    At iteration 460 -> loss: 42.83653288847554\n",
      "    At iteration 470 -> loss: 43.714247205010814\n",
      "    At iteration 480 -> loss: 44.612085473777846\n",
      "    At iteration 490 -> loss: 45.48035549587396\n",
      "    At iteration 500 -> loss: 46.339562673744496\n",
      "    At iteration 510 -> loss: 47.2011950853487\n",
      "    At iteration 520 -> loss: 48.056019369077404\n",
      "    At iteration 530 -> loss: 48.957561085574895\n",
      "    At iteration 540 -> loss: 49.80871344888092\n",
      "    At iteration 550 -> loss: 50.65936658092633\n",
      "    At iteration 560 -> loss: 51.783789480926686\n",
      "    At iteration 570 -> loss: 52.62931810975297\n",
      "    At iteration 580 -> loss: 53.50126351466625\n",
      "    At iteration 590 -> loss: 54.34398788104309\n",
      "    At iteration 600 -> loss: 55.18711871705901\n",
      "    At iteration 610 -> loss: 56.05964847010446\n",
      "    At iteration 620 -> loss: 56.97418320015541\n",
      "    At iteration 630 -> loss: 57.9599174760032\n",
      "    At iteration 640 -> loss: 58.790590186296214\n",
      "    At iteration 650 -> loss: 59.6476911054763\n",
      "    At iteration 660 -> loss: 60.54786308349347\n",
      "    At iteration 670 -> loss: 61.41214791633891\n",
      "    At iteration 680 -> loss: 62.294828055785075\n",
      "    At iteration 690 -> loss: 63.177878270939146\n",
      "    At iteration 700 -> loss: 64.0754416100938\n",
      "    At iteration 710 -> loss: 64.91974438703645\n",
      "    At iteration 720 -> loss: 65.91558209632174\n",
      "    At iteration 730 -> loss: 66.78996531712008\n",
      "    At iteration 740 -> loss: 67.64773883267597\n",
      "    At iteration 750 -> loss: 68.50293588121886\n",
      "    At iteration 760 -> loss: 69.39629899469028\n",
      "    At iteration 770 -> loss: 70.23777575772866\n",
      "    At iteration 780 -> loss: 71.10697562839087\n",
      "    At iteration 790 -> loss: 72.08047312420277\n",
      "    At iteration 800 -> loss: 73.2393454991059\n",
      "    At iteration 810 -> loss: 74.23583257759563\n",
      "    At iteration 820 -> loss: 75.11959587533309\n",
      "    At iteration 830 -> loss: 75.94961947452137\n",
      "    At iteration 840 -> loss: 77.05515695357215\n",
      "    At iteration 850 -> loss: 78.00122445716741\n",
      "    At iteration 860 -> loss: 78.8450151349289\n",
      "    At iteration 870 -> loss: 79.73890667505205\n",
      "    At iteration 880 -> loss: 80.60841339419002\n",
      "    At iteration 890 -> loss: 81.47471567016927\n",
      "    At iteration 900 -> loss: 82.38249990820495\n",
      "    At iteration 910 -> loss: 83.33181787008957\n",
      "    At iteration 920 -> loss: 84.2114074401355\n",
      "    At iteration 930 -> loss: 85.1137864174992\n",
      "    At iteration 940 -> loss: 85.96559207788536\n",
      "    At iteration 950 -> loss: 86.95398804562441\n",
      "    At iteration 960 -> loss: 87.86868211209307\n",
      "    At iteration 970 -> loss: 88.71870580222902\n",
      "    At iteration 980 -> loss: 89.57852426445324\n",
      "Staring Epoch 3\n",
      "    At iteration 0 -> loss: 0.08739142143167555\n",
      "    At iteration 10 -> loss: 0.9602376154216472\n",
      "    At iteration 20 -> loss: 2.1490406221478224\n",
      "    At iteration 30 -> loss: 2.97828000666593\n",
      "    At iteration 40 -> loss: 3.8128088876892186\n",
      "    At iteration 50 -> loss: 4.674427038404222\n",
      "    At iteration 60 -> loss: 5.612852815083873\n",
      "    At iteration 70 -> loss: 6.445967144198878\n",
      "    At iteration 80 -> loss: 7.302877084985742\n",
      "    At iteration 90 -> loss: 8.186332491729193\n",
      "    At iteration 100 -> loss: 9.23601332473547\n",
      "    At iteration 110 -> loss: 10.082344474402337\n",
      "    At iteration 120 -> loss: 10.968413861581642\n",
      "    At iteration 130 -> loss: 11.805876691269052\n",
      "    At iteration 140 -> loss: 12.658087986506164\n",
      "    At iteration 150 -> loss: 13.5314543121269\n",
      "    At iteration 160 -> loss: 14.378361460446488\n",
      "    At iteration 170 -> loss: 15.235585323570579\n",
      "    At iteration 180 -> loss: 16.19482427681871\n",
      "    At iteration 190 -> loss: 17.12184975827333\n",
      "    At iteration 200 -> loss: 17.95404189701887\n",
      "    At iteration 210 -> loss: 18.83604072532555\n",
      "    At iteration 220 -> loss: 19.732816116924344\n",
      "    At iteration 230 -> loss: 20.71239569056704\n",
      "    At iteration 240 -> loss: 21.54530696479694\n",
      "    At iteration 250 -> loss: 22.54301304791213\n",
      "    At iteration 260 -> loss: 23.375688160891034\n",
      "    At iteration 270 -> loss: 24.221039274116013\n",
      "    At iteration 280 -> loss: 25.09555867165702\n",
      "    At iteration 290 -> loss: 25.979476047101546\n",
      "    At iteration 300 -> loss: 26.948675827374448\n",
      "    At iteration 310 -> loss: 27.815520424825852\n",
      "    At iteration 320 -> loss: 28.75046282994376\n",
      "    At iteration 330 -> loss: 29.594838362123344\n",
      "    At iteration 340 -> loss: 30.46482940393057\n",
      "    At iteration 350 -> loss: 31.322844097038658\n",
      "    At iteration 360 -> loss: 32.181797899654015\n",
      "    At iteration 370 -> loss: 33.04024367632414\n",
      "    At iteration 380 -> loss: 33.98385183719412\n",
      "    At iteration 390 -> loss: 34.845747855229746\n",
      "    At iteration 400 -> loss: 35.85063484273243\n",
      "    At iteration 410 -> loss: 36.70934534629461\n",
      "    At iteration 420 -> loss: 37.57184051123451\n",
      "    At iteration 430 -> loss: 38.47835099053165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 440 -> loss: 39.320854484929185\n",
      "    At iteration 450 -> loss: 40.24665844973035\n",
      "    At iteration 460 -> loss: 41.117827001427685\n",
      "    At iteration 470 -> loss: 41.97375926894867\n",
      "    At iteration 480 -> loss: 42.8723229469758\n",
      "    At iteration 490 -> loss: 44.25165361493757\n",
      "    At iteration 500 -> loss: 45.12155214925661\n",
      "    At iteration 510 -> loss: 46.00085604090855\n",
      "    At iteration 520 -> loss: 46.86907171783448\n",
      "    At iteration 530 -> loss: 47.734456435437345\n",
      "    At iteration 540 -> loss: 48.59159002097501\n",
      "    At iteration 550 -> loss: 49.46926296800509\n",
      "    At iteration 560 -> loss: 50.32514085631035\n",
      "    At iteration 570 -> loss: 51.17560771245032\n",
      "    At iteration 580 -> loss: 52.77315549978109\n",
      "    At iteration 590 -> loss: 53.64783930789351\n",
      "    At iteration 600 -> loss: 54.54233345297441\n",
      "    At iteration 610 -> loss: 55.424175097444525\n",
      "    At iteration 620 -> loss: 56.74289724285088\n",
      "    At iteration 630 -> loss: 57.781232876373245\n",
      "    At iteration 640 -> loss: 58.62559481533142\n",
      "    At iteration 650 -> loss: 59.509943334380544\n",
      "    At iteration 660 -> loss: 60.38812261211765\n",
      "    At iteration 670 -> loss: 61.241962493839445\n",
      "    At iteration 680 -> loss: 62.11281869325298\n",
      "    At iteration 690 -> loss: 62.95142575089121\n",
      "    At iteration 700 -> loss: 63.8474327863536\n",
      "    At iteration 710 -> loss: 64.77933167442045\n",
      "    At iteration 720 -> loss: 65.67167427121392\n",
      "    At iteration 730 -> loss: 66.53781554575623\n",
      "    At iteration 740 -> loss: 67.40184734589427\n",
      "    At iteration 750 -> loss: 68.23922720823036\n",
      "    At iteration 760 -> loss: 69.21297408981779\n",
      "    At iteration 770 -> loss: 70.176823660882\n",
      "    At iteration 780 -> loss: 71.03673902263624\n",
      "    At iteration 790 -> loss: 72.09726090613155\n",
      "    At iteration 800 -> loss: 72.98085174282066\n",
      "    At iteration 810 -> loss: 76.60651209291579\n",
      "    At iteration 820 -> loss: 77.78818810348791\n",
      "    At iteration 830 -> loss: 78.62930385490624\n",
      "    At iteration 840 -> loss: 79.67065459270059\n",
      "    At iteration 850 -> loss: 80.7184683157099\n",
      "    At iteration 860 -> loss: 81.55975647783423\n",
      "    At iteration 870 -> loss: 82.4921981862514\n",
      "    At iteration 880 -> loss: 454.4062576635094\n",
      "    At iteration 890 -> loss: 457.6621014024183\n",
      "    At iteration 900 -> loss: 460.63851440477237\n",
      "    At iteration 910 -> loss: 461.8761718460643\n",
      "    At iteration 920 -> loss: 462.8509288391219\n",
      "    At iteration 930 -> loss: 463.6722952870886\n",
      "    At iteration 940 -> loss: 464.48220038274576\n",
      "    At iteration 950 -> loss: 465.3700592537498\n",
      "    At iteration 960 -> loss: 466.4451333189427\n",
      "    At iteration 970 -> loss: 467.3499830089786\n",
      "    At iteration 980 -> loss: 468.20140511812133\n",
      "Staring Epoch 4\n",
      "    At iteration 0 -> loss: 0.08271790388971567\n",
      "    At iteration 10 -> loss: 0.9106918448160286\n",
      "    At iteration 20 -> loss: 1.7426414158176158\n",
      "    At iteration 30 -> loss: 2.6917414746562827\n",
      "    At iteration 40 -> loss: 3.7210655290924706\n",
      "    At iteration 50 -> loss: 4.644462192425294\n",
      "    At iteration 60 -> loss: 5.499162027621082\n",
      "    At iteration 70 -> loss: 6.326964273034378\n",
      "    At iteration 80 -> loss: 7.612191618618112\n",
      "    At iteration 90 -> loss: 8.492254322476242\n",
      "    At iteration 100 -> loss: 9.977282574190781\n",
      "    At iteration 110 -> loss: 11.209990295895864\n",
      "    At iteration 120 -> loss: 12.047553363774227\n",
      "    At iteration 130 -> loss: 13.004836394161998\n",
      "    At iteration 140 -> loss: 13.971144510958425\n",
      "    At iteration 150 -> loss: 14.9319405602688\n",
      "    At iteration 160 -> loss: 15.804974136127953\n",
      "    At iteration 170 -> loss: 16.783434335614515\n",
      "    At iteration 180 -> loss: 17.63408197777528\n",
      "    At iteration 190 -> loss: 18.567291978985963\n",
      "    At iteration 200 -> loss: 19.44537355492423\n",
      "    At iteration 210 -> loss: 20.479816177211546\n",
      "    At iteration 220 -> loss: 21.4127232312959\n",
      "    At iteration 230 -> loss: 22.280666917205167\n",
      "    At iteration 240 -> loss: 23.1856751313818\n",
      "    At iteration 250 -> loss: 24.062058592835115\n",
      "    At iteration 260 -> loss: 24.925356120796664\n",
      "    At iteration 270 -> loss: 25.807035203857254\n",
      "    At iteration 280 -> loss: 26.6713655979853\n",
      "    At iteration 290 -> loss: 27.507804968703226\n",
      "    At iteration 300 -> loss: 28.37564870799872\n",
      "    At iteration 310 -> loss: 29.279828690897943\n",
      "    At iteration 320 -> loss: 30.167209981332803\n",
      "    At iteration 330 -> loss: 31.021122781793224\n",
      "    At iteration 340 -> loss: 31.891839411073875\n",
      "    At iteration 350 -> loss: 32.734245467813366\n",
      "    At iteration 360 -> loss: 33.608077158393485\n",
      "    At iteration 370 -> loss: 34.43423810360002\n",
      "    At iteration 380 -> loss: 35.29550088734288\n",
      "    At iteration 390 -> loss: 36.15999154655458\n",
      "    At iteration 400 -> loss: 37.058430475445334\n",
      "    At iteration 410 -> loss: 37.91227461935762\n",
      "    At iteration 420 -> loss: 38.84143622292015\n",
      "    At iteration 430 -> loss: 39.7099427650943\n",
      "    At iteration 440 -> loss: 40.76852774040583\n",
      "    At iteration 450 -> loss: 41.65065288883909\n",
      "    At iteration 460 -> loss: 42.48421695651871\n",
      "    At iteration 470 -> loss: 43.3889384728742\n",
      "    At iteration 480 -> loss: 44.23247786948798\n",
      "    At iteration 490 -> loss: 45.09941013104856\n",
      "    At iteration 500 -> loss: 45.980312913544935\n",
      "    At iteration 510 -> loss: 46.954221506127965\n",
      "    At iteration 520 -> loss: 47.83824681361659\n",
      "    At iteration 530 -> loss: 48.67050215648453\n",
      "    At iteration 540 -> loss: 49.553311755603886\n",
      "    At iteration 550 -> loss: 50.47617533887899\n",
      "    At iteration 560 -> loss: 51.310480191151896\n",
      "    At iteration 570 -> loss: 52.14450739881215\n",
      "    At iteration 580 -> loss: 53.01681953818263\n",
      "    At iteration 590 -> loss: 53.881424554890714\n",
      "    At iteration 600 -> loss: 54.80032532988628\n",
      "    At iteration 610 -> loss: 55.73710533883934\n",
      "    At iteration 620 -> loss: 56.60177071317014\n",
      "    At iteration 630 -> loss: 57.439951906105875\n",
      "    At iteration 640 -> loss: 58.310280768667894\n",
      "    At iteration 650 -> loss: 59.256527463033876\n",
      "    At iteration 660 -> loss: 60.12385792825228\n",
      "    At iteration 670 -> loss: 61.03642847280719\n",
      "    At iteration 680 -> loss: 61.90769000887269\n",
      "    At iteration 690 -> loss: 62.764980558542106\n",
      "    At iteration 700 -> loss: 63.62768963339975\n",
      "    At iteration 710 -> loss: 64.50630184697499\n",
      "    At iteration 720 -> loss: 65.36491313632857\n",
      "    At iteration 730 -> loss: 66.18978082876535\n",
      "    At iteration 740 -> loss: 67.05390737719287\n",
      "    At iteration 750 -> loss: 67.90874807083898\n",
      "    At iteration 760 -> loss: 68.77382236452874\n",
      "    At iteration 770 -> loss: 69.66002446890022\n",
      "    At iteration 780 -> loss: 70.53895962379679\n",
      "    At iteration 790 -> loss: 71.44395251282762\n",
      "    At iteration 800 -> loss: 72.33420259211957\n",
      "    At iteration 810 -> loss: 73.19614278593312\n",
      "    At iteration 820 -> loss: 74.06189903112974\n",
      "    At iteration 830 -> loss: 74.93266762066852\n",
      "    At iteration 840 -> loss: 75.78001447255085\n",
      "    At iteration 850 -> loss: 76.62190682576622\n",
      "    At iteration 860 -> loss: 77.49731388439542\n",
      "    At iteration 870 -> loss: 78.40004315348344\n",
      "    At iteration 880 -> loss: 79.34700877228684\n",
      "    At iteration 890 -> loss: 80.2459221775795\n",
      "    At iteration 900 -> loss: 81.11553466382671\n",
      "    At iteration 910 -> loss: 82.01257618424339\n",
      "    At iteration 920 -> loss: 82.8864019772233\n",
      "    At iteration 930 -> loss: 83.72622480250052\n",
      "    At iteration 940 -> loss: 84.58760448972012\n",
      "    At iteration 950 -> loss: 85.61420549208256\n",
      "    At iteration 960 -> loss: 86.49964799045543\n",
      "    At iteration 970 -> loss: 87.38706052134935\n",
      "    At iteration 980 -> loss: 88.24004413847359\n",
      "Staring Epoch 5\n",
      "    At iteration 0 -> loss: 0.08192008611513302\n",
      "    At iteration 10 -> loss: 0.952912281773024\n",
      "    At iteration 20 -> loss: 1.8476491216379145\n",
      "    At iteration 30 -> loss: 2.686815332842343\n",
      "    At iteration 40 -> loss: 3.6210169664000205\n",
      "    At iteration 50 -> loss: 4.4594492129535865\n",
      "    At iteration 60 -> loss: 5.328163865221541\n",
      "    At iteration 70 -> loss: 6.180421498046599\n",
      "    At iteration 80 -> loss: 7.004367015719254\n",
      "    At iteration 90 -> loss: 7.86687062499459\n",
      "    At iteration 100 -> loss: 8.714579547146968\n",
      "    At iteration 110 -> loss: 9.593999559101576\n",
      "    At iteration 120 -> loss: 10.508941758487708\n",
      "    At iteration 130 -> loss: 11.348664439984987\n",
      "    At iteration 140 -> loss: 12.639924878315924\n",
      "    At iteration 150 -> loss: 13.631702238449634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 160 -> loss: 14.476053221093665\n",
      "    At iteration 170 -> loss: 15.341022641989824\n",
      "    At iteration 180 -> loss: 16.198134416927772\n",
      "    At iteration 190 -> loss: 17.06788178905795\n",
      "    At iteration 200 -> loss: 17.923093800541622\n",
      "    At iteration 210 -> loss: 18.983741345504725\n",
      "    At iteration 220 -> loss: 19.84249544383279\n",
      "    At iteration 230 -> loss: 20.719975479988037\n",
      "    At iteration 240 -> loss: 21.611680185506387\n",
      "    At iteration 250 -> loss: 22.4588642009589\n",
      "    At iteration 260 -> loss: 23.32227549350212\n",
      "    At iteration 270 -> loss: 24.42025042155167\n",
      "    At iteration 280 -> loss: 25.362979412182383\n",
      "    At iteration 290 -> loss: 26.192025716389097\n",
      "    At iteration 300 -> loss: 27.053723754333316\n",
      "    At iteration 310 -> loss: 27.893650177983783\n",
      "    At iteration 320 -> loss: 28.755012954542746\n",
      "    At iteration 330 -> loss: 29.62198884418218\n",
      "    At iteration 340 -> loss: 30.470256705630288\n",
      "    At iteration 350 -> loss: 31.398054224446366\n",
      "    At iteration 360 -> loss: 32.254227252757246\n",
      "    At iteration 370 -> loss: 33.12166252695182\n",
      "    At iteration 380 -> loss: 34.574661099111964\n",
      "    At iteration 390 -> loss: 35.511048697071274\n",
      "    At iteration 400 -> loss: 36.463254551855826\n",
      "    At iteration 410 -> loss: 38.01291848418772\n",
      "    At iteration 420 -> loss: 38.861761714178684\n",
      "    At iteration 430 -> loss: 39.73950882237081\n",
      "    At iteration 440 -> loss: 40.582376093836174\n",
      "    At iteration 450 -> loss: 41.45094183175147\n",
      "    At iteration 460 -> loss: 42.32311772811713\n",
      "    At iteration 470 -> loss: 43.21470568063669\n",
      "    At iteration 480 -> loss: 44.15443271043805\n",
      "    At iteration 490 -> loss: 45.062804259108454\n",
      "    At iteration 500 -> loss: 45.9457555164041\n",
      "    At iteration 510 -> loss: 46.821623530983494\n",
      "    At iteration 520 -> loss: 47.70615153369611\n",
      "    At iteration 530 -> loss: 48.61990312917508\n",
      "    At iteration 540 -> loss: 49.48947454498805\n",
      "    At iteration 550 -> loss: 50.38701071109856\n",
      "    At iteration 560 -> loss: 51.546261690177495\n",
      "    At iteration 570 -> loss: 52.4471872064157\n",
      "    At iteration 580 -> loss: 53.30482929224738\n",
      "    At iteration 590 -> loss: 54.17107013472626\n",
      "    At iteration 600 -> loss: 55.01016806752953\n",
      "    At iteration 610 -> loss: 55.88829352721289\n",
      "    At iteration 620 -> loss: 56.76479810891008\n",
      "    At iteration 630 -> loss: 57.64658423095509\n",
      "    At iteration 640 -> loss: 58.52864421690149\n",
      "    At iteration 650 -> loss: 59.435900706659346\n",
      "    At iteration 660 -> loss: 60.3208392832855\n",
      "    At iteration 670 -> loss: 61.20041999101533\n",
      "    At iteration 680 -> loss: 62.05236334435187\n",
      "    At iteration 690 -> loss: 62.93774853205843\n",
      "    At iteration 700 -> loss: 63.78720454419237\n",
      "    At iteration 710 -> loss: 64.67415501315247\n",
      "    At iteration 720 -> loss: 65.5004942511556\n",
      "    At iteration 730 -> loss: 66.37502551368073\n",
      "    At iteration 740 -> loss: 67.22801067404782\n",
      "    At iteration 750 -> loss: 68.17219268812607\n",
      "    At iteration 760 -> loss: 69.05937352097374\n",
      "    At iteration 770 -> loss: 69.96632479961215\n",
      "    At iteration 780 -> loss: 70.837379565064\n",
      "    At iteration 790 -> loss: 71.69025704225521\n",
      "    At iteration 800 -> loss: 72.63011481723686\n",
      "    At iteration 810 -> loss: 73.45940129503322\n",
      "    At iteration 820 -> loss: 74.35953315745826\n",
      "    At iteration 830 -> loss: 75.22630621261158\n",
      "    At iteration 840 -> loss: 76.17773864397566\n",
      "    At iteration 850 -> loss: 77.06975995292339\n",
      "    At iteration 860 -> loss: 77.93259595197958\n",
      "    At iteration 870 -> loss: 78.83653227149841\n",
      "    At iteration 880 -> loss: 79.7451485793157\n",
      "    At iteration 890 -> loss: 80.65915663977137\n",
      "    At iteration 900 -> loss: 81.487419229961\n",
      "    At iteration 910 -> loss: 82.42170646603313\n",
      "    At iteration 920 -> loss: 83.45931611195772\n",
      "    At iteration 930 -> loss: 84.32629687422775\n",
      "    At iteration 940 -> loss: 85.17116540879692\n",
      "    At iteration 950 -> loss: 86.29045530066401\n",
      "    At iteration 960 -> loss: 87.16418738200286\n",
      "    At iteration 970 -> loss: 88.05676248131044\n",
      "    At iteration 980 -> loss: 88.91253885943865\n",
      "Staring Epoch 6\n",
      "    At iteration 0 -> loss: 0.08462002163287252\n",
      "    At iteration 10 -> loss: 0.9357635318956454\n",
      "    At iteration 20 -> loss: 1.8960714990334964\n",
      "    At iteration 30 -> loss: 2.7673745002193755\n",
      "    At iteration 40 -> loss: 3.626219029445565\n",
      "    At iteration 50 -> loss: 4.494055050706066\n",
      "    At iteration 60 -> loss: 5.372713933478536\n",
      "    At iteration 70 -> loss: 6.277596805933882\n",
      "    At iteration 80 -> loss: 7.102374020314329\n",
      "    At iteration 90 -> loss: 8.025070161171044\n",
      "    At iteration 100 -> loss: 8.88790617175053\n",
      "    At iteration 110 -> loss: 9.806254778455497\n",
      "    At iteration 120 -> loss: 10.89001836935293\n",
      "    At iteration 130 -> loss: 12.02489190272693\n",
      "    At iteration 140 -> loss: 12.962236019566243\n",
      "    At iteration 150 -> loss: 13.797447644111324\n",
      "    At iteration 160 -> loss: 14.665826940404239\n",
      "    At iteration 170 -> loss: 15.597246370733936\n",
      "    At iteration 180 -> loss: 16.537833596424186\n",
      "    At iteration 190 -> loss: 17.446814293199147\n",
      "    At iteration 200 -> loss: 18.2949522757526\n",
      "    At iteration 210 -> loss: 19.68916859202816\n",
      "    At iteration 220 -> loss: 20.544305691911525\n",
      "    At iteration 230 -> loss: 21.444707864452887\n",
      "    At iteration 240 -> loss: 22.31058031726696\n",
      "    At iteration 250 -> loss: 23.216088044327428\n",
      "    At iteration 260 -> loss: 24.07197162616547\n",
      "    At iteration 270 -> loss: 24.945435017819257\n",
      "    At iteration 280 -> loss: 25.860680978998865\n",
      "    At iteration 290 -> loss: 26.69189839623653\n",
      "    At iteration 300 -> loss: 27.633988951781937\n",
      "    At iteration 310 -> loss: 28.474556841601732\n",
      "    At iteration 320 -> loss: 29.35646628318387\n",
      "    At iteration 330 -> loss: 30.234636308042134\n",
      "    At iteration 340 -> loss: 31.09168840840147\n",
      "    At iteration 350 -> loss: 31.969686554629963\n",
      "    At iteration 360 -> loss: 32.84158705249291\n",
      "    At iteration 370 -> loss: 33.7330440234652\n",
      "    At iteration 380 -> loss: 34.95171068773897\n",
      "    At iteration 390 -> loss: 35.81175567071381\n",
      "    At iteration 400 -> loss: 36.69522931617047\n",
      "    At iteration 410 -> loss: 37.593513691306896\n",
      "    At iteration 420 -> loss: 38.435696383721904\n",
      "    At iteration 430 -> loss: 39.29000503070546\n",
      "    At iteration 440 -> loss: 40.18525413857623\n",
      "    At iteration 450 -> loss: 41.15694498995998\n",
      "    At iteration 460 -> loss: 42.103767922553914\n",
      "    At iteration 470 -> loss: 42.93740888351584\n",
      "    At iteration 480 -> loss: 43.809694269495765\n",
      "    At iteration 490 -> loss: 44.88198537107854\n",
      "    At iteration 500 -> loss: 45.70900046407138\n",
      "    At iteration 510 -> loss: 46.58537971321306\n",
      "    At iteration 520 -> loss: 47.46881410656164\n",
      "    At iteration 530 -> loss: 48.34762846363259\n",
      "    At iteration 540 -> loss: 49.18793392945548\n",
      "    At iteration 550 -> loss: 50.038743469967756\n",
      "    At iteration 560 -> loss: 50.95407132313443\n",
      "    At iteration 570 -> loss: 51.8048107682643\n",
      "    At iteration 580 -> loss: 52.89063584174477\n",
      "    At iteration 590 -> loss: 53.83953294729366\n",
      "    At iteration 600 -> loss: 54.736601595674365\n",
      "    At iteration 610 -> loss: 55.582619291771735\n",
      "    At iteration 620 -> loss: 56.45886970274809\n",
      "    At iteration 630 -> loss: 57.34086870442992\n",
      "    At iteration 640 -> loss: 58.539839321040645\n",
      "    At iteration 650 -> loss: 59.37564263141293\n",
      "    At iteration 660 -> loss: 60.25969032805632\n",
      "    At iteration 670 -> loss: 61.11834506715867\n",
      "    At iteration 680 -> loss: 61.956076438330356\n",
      "    At iteration 690 -> loss: 62.838206516794045\n",
      "    At iteration 700 -> loss: 63.74768841317261\n",
      "    At iteration 710 -> loss: 64.95685976459269\n",
      "    At iteration 720 -> loss: 65.84276883660202\n",
      "    At iteration 730 -> loss: 66.74383894867348\n",
      "    At iteration 740 -> loss: 67.61631536786321\n",
      "    At iteration 750 -> loss: 68.46791282519054\n",
      "    At iteration 760 -> loss: 69.3500218608847\n",
      "    At iteration 770 -> loss: 70.20230600216611\n",
      "    At iteration 780 -> loss: 71.13525712155617\n",
      "    At iteration 790 -> loss: 72.2051329977353\n",
      "    At iteration 800 -> loss: 73.09297672717595\n",
      "    At iteration 810 -> loss: 74.45554632431096\n",
      "    At iteration 820 -> loss: 75.36299862457282\n",
      "    At iteration 830 -> loss: 76.28935812883402\n",
      "    At iteration 840 -> loss: 77.14196713445064\n",
      "    At iteration 850 -> loss: 78.02252079560687\n",
      "    At iteration 860 -> loss: 78.92266158439135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 870 -> loss: 79.78166817677221\n",
      "    At iteration 880 -> loss: 80.6532128435497\n",
      "    At iteration 890 -> loss: 81.49659543200808\n",
      "    At iteration 900 -> loss: 82.37534198202896\n",
      "    At iteration 910 -> loss: 83.23007997477664\n",
      "    At iteration 920 -> loss: 84.09389960707752\n",
      "    At iteration 930 -> loss: 84.97817911818068\n",
      "    At iteration 940 -> loss: 85.82787378894075\n",
      "    At iteration 950 -> loss: 86.68945833413484\n",
      "    At iteration 960 -> loss: 87.56553324876229\n",
      "    At iteration 970 -> loss: 88.43924914521274\n",
      "    At iteration 980 -> loss: 89.31355457646357\n",
      "Staring Epoch 7\n",
      "    At iteration 0 -> loss: 0.08491190988570452\n",
      "    At iteration 10 -> loss: 0.9253234866446292\n",
      "    At iteration 20 -> loss: 1.7796572095162009\n",
      "    At iteration 30 -> loss: 3.4432699849625825\n",
      "    At iteration 40 -> loss: 4.277547312330582\n",
      "    At iteration 50 -> loss: 5.148351403452991\n",
      "    At iteration 60 -> loss: 5.9913262325419\n",
      "    At iteration 70 -> loss: 6.889002527800762\n",
      "    At iteration 80 -> loss: 7.73463320125365\n",
      "    At iteration 90 -> loss: 8.597101431617098\n",
      "    At iteration 100 -> loss: 9.437941034239607\n",
      "    At iteration 110 -> loss: 10.309170653718589\n",
      "    At iteration 120 -> loss: 11.137790077117366\n",
      "    At iteration 130 -> loss: 12.0155213273435\n",
      "    At iteration 140 -> loss: 12.854724077484384\n",
      "    At iteration 150 -> loss: 13.730445468450121\n",
      "    At iteration 160 -> loss: 14.567680544477476\n",
      "    At iteration 170 -> loss: 15.448642846512662\n",
      "    At iteration 180 -> loss: 16.351827060886713\n",
      "    At iteration 190 -> loss: 17.28697878713428\n",
      "    At iteration 200 -> loss: 18.23071083860777\n",
      "    At iteration 210 -> loss: 19.724353377684743\n",
      "    At iteration 220 -> loss: 20.551102506369894\n",
      "    At iteration 230 -> loss: 21.57257851471713\n",
      "    At iteration 240 -> loss: 22.4858672073375\n",
      "    At iteration 250 -> loss: 23.36509142555804\n",
      "    At iteration 260 -> loss: 24.30077551595489\n",
      "    At iteration 270 -> loss: 25.160747306718555\n",
      "    At iteration 280 -> loss: 26.012854735430416\n",
      "    At iteration 290 -> loss: 26.86679976222488\n",
      "    At iteration 300 -> loss: 27.8502453394319\n",
      "    At iteration 310 -> loss: 28.685494382821823\n",
      "    At iteration 320 -> loss: 29.562546683280495\n",
      "    At iteration 330 -> loss: 30.417321694009594\n",
      "    At iteration 340 -> loss: 31.28868770722213\n",
      "    At iteration 350 -> loss: 32.21607591323002\n",
      "    At iteration 360 -> loss: 33.055869175884006\n",
      "    At iteration 370 -> loss: 33.933064513241995\n",
      "    At iteration 380 -> loss: 34.95447468927771\n",
      "    At iteration 390 -> loss: 35.89554787746486\n",
      "    At iteration 400 -> loss: 36.774641032777005\n",
      "    At iteration 410 -> loss: 37.680268805221864\n",
      "    At iteration 420 -> loss: 38.52341103285537\n",
      "    At iteration 430 -> loss: 39.42042223111887\n",
      "    At iteration 440 -> loss: 40.37794782884405\n",
      "    At iteration 450 -> loss: 41.2768711783526\n",
      "    At iteration 460 -> loss: 42.138968711564964\n",
      "    At iteration 470 -> loss: 42.98340236982299\n",
      "    At iteration 480 -> loss: 43.87412928265881\n",
      "    At iteration 490 -> loss: 44.948780864013635\n",
      "    At iteration 500 -> loss: 45.92001918478508\n",
      "    At iteration 510 -> loss: 46.77492648592324\n",
      "    At iteration 520 -> loss: 47.710079971755654\n",
      "    At iteration 530 -> loss: 48.58344176340342\n",
      "    At iteration 540 -> loss: 49.45571115834918\n",
      "    At iteration 550 -> loss: 50.346231048177465\n",
      "    At iteration 560 -> loss: 51.21907220383418\n",
      "    At iteration 570 -> loss: 52.0701480561167\n",
      "    At iteration 580 -> loss: 52.965211262822955\n",
      "    At iteration 590 -> loss: 53.98233779241691\n",
      "    At iteration 600 -> loss: 54.867608072278195\n",
      "    At iteration 610 -> loss: 55.80450051149655\n",
      "    At iteration 620 -> loss: 56.73903672200522\n",
      "    At iteration 630 -> loss: 57.68143283690569\n",
      "    At iteration 640 -> loss: 58.55558170311105\n",
      "    At iteration 650 -> loss: 59.45273246817869\n",
      "    At iteration 660 -> loss: 60.32462274404151\n",
      "    At iteration 670 -> loss: 61.26143154101573\n",
      "    At iteration 680 -> loss: 62.16598202266159\n",
      "    At iteration 690 -> loss: 63.049566940271866\n",
      "    At iteration 700 -> loss: 64.00844376617732\n",
      "    At iteration 710 -> loss: 64.88979457093791\n",
      "    At iteration 720 -> loss: 65.77714789730652\n",
      "    At iteration 730 -> loss: 66.63166632597694\n",
      "    At iteration 740 -> loss: 67.52540696594059\n",
      "    At iteration 750 -> loss: 68.39515926134317\n",
      "    At iteration 760 -> loss: 69.38708401181628\n",
      "    At iteration 770 -> loss: 70.62685214998481\n",
      "    At iteration 780 -> loss: 71.57654668819731\n",
      "    At iteration 790 -> loss: 72.42623665020591\n",
      "    At iteration 800 -> loss: 73.3060021015387\n",
      "    At iteration 810 -> loss: 74.14680565203679\n",
      "    At iteration 820 -> loss: 75.00703086508355\n",
      "    At iteration 830 -> loss: 75.85688314276803\n",
      "    At iteration 840 -> loss: 76.75995928333094\n",
      "    At iteration 850 -> loss: 77.60248378811474\n",
      "    At iteration 860 -> loss: 78.45718271211325\n",
      "    At iteration 870 -> loss: 79.3133929265702\n",
      "    At iteration 880 -> loss: 80.19223492788541\n",
      "    At iteration 890 -> loss: 81.06099847286004\n",
      "    At iteration 900 -> loss: 82.03371452633831\n",
      "    At iteration 910 -> loss: 82.87385150534641\n",
      "    At iteration 920 -> loss: 83.75498124748077\n",
      "    At iteration 930 -> loss: 84.66452488830134\n",
      "    At iteration 940 -> loss: 85.80246326921213\n",
      "    At iteration 950 -> loss: 86.73040014554556\n",
      "    At iteration 960 -> loss: 87.58451028202091\n",
      "    At iteration 970 -> loss: 88.43845575486614\n",
      "    At iteration 980 -> loss: 89.38698999016617\n",
      "Staring Epoch 8\n",
      "    At iteration 0 -> loss: 0.08059105620486662\n",
      "    At iteration 10 -> loss: 0.9753298786567939\n",
      "    At iteration 20 -> loss: 1.8017077506124224\n",
      "    At iteration 30 -> loss: 2.6987858751713247\n",
      "    At iteration 40 -> loss: 3.561262704872661\n",
      "    At iteration 50 -> loss: 4.443217568656053\n",
      "    At iteration 60 -> loss: 5.2863972418402625\n",
      "    At iteration 70 -> loss: 6.203145883566634\n",
      "    At iteration 80 -> loss: 7.019464942745202\n",
      "    At iteration 90 -> loss: 7.9924071437002056\n",
      "    At iteration 100 -> loss: 8.846639883604766\n",
      "    At iteration 110 -> loss: 9.714090706506212\n",
      "    At iteration 120 -> loss: 10.606207419843813\n",
      "    At iteration 130 -> loss: 11.640448440072177\n",
      "    At iteration 140 -> loss: 12.513959686953385\n",
      "    At iteration 150 -> loss: 13.528858404589627\n",
      "    At iteration 160 -> loss: 14.4466793798682\n",
      "    At iteration 170 -> loss: 15.343476709346632\n",
      "    At iteration 180 -> loss: 16.232417105978\n",
      "    At iteration 190 -> loss: 17.085314883369264\n",
      "    At iteration 200 -> loss: 18.00211399116857\n",
      "    At iteration 210 -> loss: 18.83976807512032\n",
      "    At iteration 220 -> loss: 19.73475967611226\n",
      "    At iteration 230 -> loss: 20.577629049280617\n",
      "    At iteration 240 -> loss: 21.46227143511691\n",
      "    At iteration 250 -> loss: 22.350635761027426\n",
      "    At iteration 260 -> loss: 23.723219971139535\n",
      "    At iteration 270 -> loss: 24.5997170863809\n",
      "    At iteration 280 -> loss: 25.53049354778681\n",
      "    At iteration 290 -> loss: 26.40313206642508\n",
      "    At iteration 300 -> loss: 27.237469263210862\n",
      "    At iteration 310 -> loss: 28.098287813388872\n",
      "    At iteration 320 -> loss: 28.949452712345867\n",
      "    At iteration 330 -> loss: 29.846309213695115\n",
      "    At iteration 340 -> loss: 30.69344787624845\n",
      "    At iteration 350 -> loss: 31.584360325624168\n",
      "    At iteration 360 -> loss: 32.42991754996432\n",
      "    At iteration 370 -> loss: 33.27127845309473\n",
      "    At iteration 380 -> loss: 34.148485057003874\n",
      "    At iteration 390 -> loss: 35.2127830590241\n",
      "    At iteration 400 -> loss: 36.06320356340574\n",
      "    At iteration 410 -> loss: 37.038080230545795\n",
      "    At iteration 420 -> loss: 37.92341411561892\n",
      "    At iteration 430 -> loss: 38.80177540346585\n",
      "    At iteration 440 -> loss: 39.64800982773112\n",
      "    At iteration 450 -> loss: 40.542913475822836\n",
      "    At iteration 460 -> loss: 41.41163122466627\n",
      "    At iteration 470 -> loss: 42.4793190747589\n",
      "    At iteration 480 -> loss: 43.583352813624735\n",
      "    At iteration 490 -> loss: 44.641895736498775\n",
      "    At iteration 500 -> loss: 45.478111239368666\n",
      "    At iteration 510 -> loss: 46.41305362799471\n",
      "    At iteration 520 -> loss: 47.25590975974341\n",
      "    At iteration 530 -> loss: 48.29038948846868\n",
      "    At iteration 540 -> loss: 49.154110782273385\n",
      "    At iteration 550 -> loss: 50.02881440405621\n",
      "    At iteration 560 -> loss: 51.49058343258297\n",
      "    At iteration 570 -> loss: 52.39092542220828\n",
      "    At iteration 580 -> loss: 53.284661415352495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 590 -> loss: 54.12999848431286\n",
      "    At iteration 600 -> loss: 54.993153573515194\n",
      "    At iteration 610 -> loss: 55.885937073545435\n",
      "    At iteration 620 -> loss: 56.76033731082957\n",
      "    At iteration 630 -> loss: 57.67215807928828\n",
      "    At iteration 640 -> loss: 58.68311557459769\n",
      "    At iteration 650 -> loss: 59.63169033779654\n",
      "    At iteration 660 -> loss: 61.03483697655149\n",
      "    At iteration 670 -> loss: 61.94875870447383\n",
      "    At iteration 680 -> loss: 62.84062164387676\n",
      "    At iteration 690 -> loss: 63.68393437193542\n",
      "    At iteration 700 -> loss: 64.5827504841315\n",
      "    At iteration 710 -> loss: 65.49627830621164\n",
      "    At iteration 720 -> loss: 66.46045057390863\n",
      "    At iteration 730 -> loss: 67.32967301116211\n",
      "    At iteration 740 -> loss: 68.21442308453031\n",
      "    At iteration 750 -> loss: 69.08842097665972\n",
      "    At iteration 760 -> loss: 69.99474289217385\n",
      "    At iteration 770 -> loss: 70.87731260758362\n",
      "    At iteration 780 -> loss: 71.74315622636702\n",
      "    At iteration 790 -> loss: 72.61142047126548\n",
      "    At iteration 800 -> loss: 73.45675284937685\n",
      "    At iteration 810 -> loss: 74.3033519685881\n",
      "    At iteration 820 -> loss: 75.23858763091572\n",
      "    At iteration 830 -> loss: 76.06590152818113\n",
      "    At iteration 840 -> loss: 76.99680944181533\n",
      "    At iteration 850 -> loss: 78.00555740763126\n",
      "    At iteration 860 -> loss: 78.84606436754676\n",
      "    At iteration 870 -> loss: 79.73966533974145\n",
      "    At iteration 880 -> loss: 80.60949015902371\n",
      "    At iteration 890 -> loss: 81.46346391139888\n",
      "    At iteration 900 -> loss: 82.30763643533497\n",
      "    At iteration 910 -> loss: 83.16081172839091\n",
      "    At iteration 920 -> loss: 84.06771122245169\n",
      "    At iteration 930 -> loss: 84.95361285808082\n",
      "    At iteration 940 -> loss: 85.81389139210347\n",
      "    At iteration 950 -> loss: 86.70715412646729\n",
      "    At iteration 960 -> loss: 87.59596886565174\n",
      "    At iteration 970 -> loss: 88.46308551811904\n",
      "    At iteration 980 -> loss: 89.33613364164427\n",
      "Staring Epoch 9\n",
      "    At iteration 0 -> loss: 0.08160626911558211\n",
      "    At iteration 10 -> loss: 0.94532771698141\n",
      "    At iteration 20 -> loss: 1.7792991171045287\n",
      "    At iteration 30 -> loss: 2.6798936099548882\n",
      "    At iteration 40 -> loss: 3.514666826681264\n",
      "    At iteration 50 -> loss: 4.40501099639971\n",
      "    At iteration 60 -> loss: 5.240443409332897\n",
      "    At iteration 70 -> loss: 6.466265389200657\n",
      "    At iteration 80 -> loss: 7.298290804771796\n",
      "    At iteration 90 -> loss: 8.17722178981353\n",
      "    At iteration 100 -> loss: 9.07051250833615\n",
      "    At iteration 110 -> loss: 9.944139157728841\n",
      "    At iteration 120 -> loss: 10.836940640774174\n",
      "    At iteration 130 -> loss: 12.207017709458796\n",
      "    At iteration 140 -> loss: 13.221890754678952\n",
      "    At iteration 150 -> loss: 14.125407361805173\n",
      "    At iteration 160 -> loss: 15.03718493189399\n",
      "    At iteration 170 -> loss: 16.051556242279958\n",
      "    At iteration 180 -> loss: 16.97293007125097\n",
      "    At iteration 190 -> loss: 17.827205118700476\n",
      "    At iteration 200 -> loss: 18.91595417859844\n",
      "    At iteration 210 -> loss: 19.87544362042445\n",
      "    At iteration 220 -> loss: 20.755650693044117\n",
      "    At iteration 230 -> loss: 21.70314690437499\n",
      "    At iteration 240 -> loss: 22.760202464082795\n",
      "    At iteration 250 -> loss: 23.743901323689897\n",
      "    At iteration 260 -> loss: 24.600113544892338\n",
      "    At iteration 270 -> loss: 25.52793007584387\n",
      "    At iteration 280 -> loss: 26.451087311201263\n",
      "    At iteration 290 -> loss: 27.34084298007656\n",
      "    At iteration 300 -> loss: 28.244045146753706\n",
      "    At iteration 310 -> loss: 29.078876147317033\n",
      "    At iteration 320 -> loss: 29.973664055507697\n",
      "    At iteration 330 -> loss: 30.848412812562174\n",
      "    At iteration 340 -> loss: 31.69484266485074\n",
      "    At iteration 350 -> loss: 32.56301085908987\n",
      "    At iteration 360 -> loss: 33.411193113920916\n",
      "    At iteration 370 -> loss: 34.29804265592381\n",
      "    At iteration 380 -> loss: 35.13133457198407\n",
      "    At iteration 390 -> loss: 36.049143409095876\n",
      "    At iteration 400 -> loss: 36.97374044319574\n",
      "    At iteration 410 -> loss: 37.8256837589374\n",
      "    At iteration 420 -> loss: 38.65751523983769\n",
      "    At iteration 430 -> loss: 39.52068956364516\n",
      "    At iteration 440 -> loss: 40.39481470849702\n",
      "    At iteration 450 -> loss: 41.27739576523297\n",
      "    At iteration 460 -> loss: 42.117258507868854\n",
      "    At iteration 470 -> loss: 42.99119395230579\n",
      "    At iteration 480 -> loss: 43.8509730179189\n",
      "    At iteration 490 -> loss: 44.71386967575151\n",
      "    At iteration 500 -> loss: 45.595610318403914\n",
      "    At iteration 510 -> loss: 46.483967743421516\n",
      "    At iteration 520 -> loss: 47.458214170067066\n",
      "    At iteration 530 -> loss: 48.3029950559289\n",
      "    At iteration 540 -> loss: 49.194734332676006\n",
      "    At iteration 550 -> loss: 50.10247335163555\n",
      "    At iteration 560 -> loss: 51.07616416991047\n",
      "    At iteration 570 -> loss: 51.92008431452311\n",
      "    At iteration 580 -> loss: 52.84948390301767\n",
      "    At iteration 590 -> loss: 53.69535738817328\n",
      "    At iteration 600 -> loss: 54.714338608643146\n",
      "    At iteration 610 -> loss: 55.669103166849226\n",
      "    At iteration 620 -> loss: 56.51655315711511\n",
      "    At iteration 630 -> loss: 58.252401350492136\n",
      "    At iteration 640 -> loss: 59.18049863413441\n",
      "    At iteration 650 -> loss: 60.065357607469814\n",
      "    At iteration 660 -> loss: 61.021002944922365\n",
      "    At iteration 670 -> loss: 61.85746664456354\n",
      "    At iteration 680 -> loss: 62.730120873840626\n",
      "    At iteration 690 -> loss: 63.5786916750419\n",
      "    At iteration 700 -> loss: 64.45948432542482\n",
      "    At iteration 710 -> loss: 65.3859767847047\n",
      "    At iteration 720 -> loss: 66.23708247618376\n",
      "    At iteration 730 -> loss: 67.07345575534598\n",
      "    At iteration 740 -> loss: 67.95809103852113\n",
      "    At iteration 750 -> loss: 68.79270634103406\n",
      "    At iteration 760 -> loss: 69.6820723009175\n",
      "    At iteration 770 -> loss: 70.54408746345038\n",
      "    At iteration 780 -> loss: 71.41584443038195\n",
      "    At iteration 790 -> loss: 72.26332609348796\n",
      "    At iteration 800 -> loss: 73.1700564070405\n",
      "    At iteration 810 -> loss: 74.01492747181314\n",
      "    At iteration 820 -> loss: 75.04633780667777\n",
      "    At iteration 830 -> loss: 75.95690630844169\n",
      "    At iteration 840 -> loss: 76.86226603847885\n",
      "    At iteration 850 -> loss: 77.70315527431731\n",
      "    At iteration 860 -> loss: 78.54865832081478\n",
      "    At iteration 870 -> loss: 79.58435931108025\n",
      "    At iteration 880 -> loss: 80.48893533008587\n",
      "    At iteration 890 -> loss: 81.38440271155915\n",
      "    At iteration 900 -> loss: 82.44644588280417\n",
      "    At iteration 910 -> loss: 83.3410045479176\n",
      "    At iteration 920 -> loss: 84.18330900392712\n",
      "    At iteration 930 -> loss: 85.04650951635077\n",
      "    At iteration 940 -> loss: 85.91572633902061\n",
      "    At iteration 950 -> loss: 86.80080170188549\n",
      "    At iteration 960 -> loss: 87.64045533540562\n",
      "    At iteration 970 -> loss: 88.49606670478502\n",
      "    At iteration 980 -> loss: 89.34525298933991\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(f\"Staring Epoch {epoch}\")\n",
    "    model.to(device)\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    for i, (centers, lefts, rights) in enumerate(data_generator):\n",
    "        centers, lefts, rights = toDevice(centers, device), toDevice(lefts, device), toDevice(rights, device)\n",
    "        optimizer.zero_grad()\n",
    "        datas = [centers, lefts, rights]\n",
    "        for data in datas:\n",
    "            imgs, angles = data\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, angles.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.data.item()\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            print(f\"    At iteration {i} -> loss: {train_loss / (i+1)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.085 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.096 \n",
      "Valid Loss: 0.095 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.091 \n",
      "Valid Loss: 0.091 \n",
      "Valid Loss: 0.091 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.088 \n",
      "Valid Loss: 0.088 \n",
      "Valid Loss: 0.088 \n",
      "Valid Loss: 0.088 \n",
      "Valid Loss: 0.088 \n",
      "Valid Loss: 0.088 \n",
      "Valid Loss: 0.088 \n",
      "Valid Loss: 0.088 \n",
      "Valid Loss: 0.088 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "valid_loss = 0\n",
    "with torch.set_grad_enabled(False):\n",
    "    for local_batch, (centers, lefts, rights) in enumerate(data_generator):\n",
    "        # Transfer to GPU\n",
    "        centers, lefts, rights = toDevice(centers, device), toDevice(lefts, device), toDevice(rights, device)\n",
    "\n",
    "        # Model computations\n",
    "        optimizer.zero_grad()\n",
    "        datas = [centers, lefts, rights]        \n",
    "        for data in datas:\n",
    "            imgs, angles = data\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, angles.unsqueeze(1))\n",
    "\n",
    "            valid_loss += loss.data.item()\n",
    "\n",
    "        if local_batch % 10 == 0:\n",
    "            print('Valid Loss: %.3f '\n",
    "                 % (valid_loss/(local_batch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step8: Define state and save the model wrt to state\n",
    "# state = {\n",
    "#         'model': model.module if device == 'cuda' else model,\n",
    "#         }\n",
    "\n",
    "# torch.save(state, 'model.h5')\n",
    "torch.save(model, 'model.h5')\n",
    "# torch.save(model.state_dict(), 'state_dict.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
